---
title: "Data reduction of biphytane d2H measurements"
subtitle: "experimental testing"
date: "`r format(Sys.Date(), '%d %b %Y')`"
number-sections: true
number-offset: 0
number-depth: 1
toc: true
toc-depth: 2
fig-width: 6
fig-height: 4
df-print: tibble
embed-resources: true
format: 
  html: 
    code-tools: true
    code-fold: true
    code-summary: "Show the code"
    toc-float: true
  docx: 
    toc-title: "Table of contents"
    fig-dpi: 600
    execute:
      echo: false
knitr: 
  opts_chunk: 
    fig.path: "plots/data_reduction-wnls-"
    fig.keep: "all"
    dev: ['png', 'pdf']
    dev.args: 
      pdf: 
        encoding: 'WinAnsi'
        useDingbats: false
crossref:
  fig-prefix: Fig.
  tbl-prefix: Table
  ref-hyperlink: true
editor: source
editor_options: 
  chunk_output_type: console
---

Using `r R.version.string`, tidyverse version `r packageVersion("tidyverse")`, isoreader version `r packageVersion("isoreader")`, and isoprocessor version `r packageVersion("isoprocessor")`.

```{r}
#| label: setup
#| echo: true
#| message: false
#| warning: false

# load libraries
library(tidyverse)
library(isoprocessor)

# load scripts
source("scripts/table_functions.R")
source("scripts/plotting_functions.R")
```

# IMPORTANT: experimental, non-nls approach is sufficient

> Note: the first code block in sections marked with (*) are essential, everything else is additional plotting and/or analysis to explore the data.

# Approach

Assume a background `bgrd` signal contributes to each analyte `x` such that the measured ratio for any `peak` relative to the `H2` reference gas is the mass balance between `x` and `bgrd` (note that in GC-IRMS, `bgrd` is not usually an actual blank because the baseline is already subtracted at the time of peak integration but there are other effects that conceptually are like a `bgrd` contribution such as memory effects from H sorbed in the system and potentially from the pyrolysis reactor):

$$
\begin{aligned}
\frac{R_{peak}}{R_{H_2}} 
  &=\frac{A_{x}}{A_{peak}}\cdot\frac{R_{analyte}}{R_{H_2}}+\frac{A_{bgrd}}{A_{peak}}\cdot\frac{R_{blank}}{R_{H_2}} \\
  &= \left( \frac{A_{peak} - A_{bgrd}}{A_{peak}}\cdot\frac{R_{analyte}}{R_{VSMOW}} 
  + \frac{A_{bgrd}}{A_{peak}}\cdot\frac{R_{blank}}{R_{VSMOW}} \right) \cdot{} \frac{R_{VSMOW}}{R_{H_2}} \\
\delta_{peak/H_2}  &= \left( \left( 1 - \frac{A_{bgrd}}{A_{peak}} \right) \cdot (\delta_x + 1)
  + \frac{A_{bgrd}}{A_{peak}}\cdot (\delta_{bgrd} + 1) \right) \cdot\frac{R_{VSMOW}}{R_{H_2}} - 1 \\
\end{aligned}
$$

This mass balance has the following properties:

 - we know $\delta_x$ for standards and want to figure out its values for unknown analytes
 - we have 2 quantities we measure: $\delta_{peak/H_2}$ and $A_{peak}$
 - we have 3 unknowns that we assume to be constant in a single analytical session: 
  - $R_{VSMOW}/R_{H_2}$: relates to the isotopic composition $\delta_{H2}$ of our reference gas and inherently includes any fractionation between analytes and ref gas that happen during the analysis
  - $A_{bgrd}$: the amount that the bgrd contributes to each peak
  - $\delta_{bgrd}$: the isotopic composition of the bgrd

> notation notes 

 - all $\delta_x$ values that do not have an explicit denominator ($\delta_{x/y}$) are vs. VSMOW
 - all $\delta$ values are implicitly multiplied by 1000 when reported as ‰ and must be divided by 1000 when in ‰ before being used in equations

To figure out the 3 unknown constants, we can take the following approach (starting at Step 1): 

## Step 0: $\delta_{peak/H_2} = \beta_0 + \beta_1 \cdot \delta_x$

This is not useful for getting at the parameters but helpful to get a quick first glance at all the standards. Assuming a single standard has similar peak areas, the below univariate linear regression allows assessment of parameter estimates for each individual standard analysis:

$$
\delta_{peak/H_2} = \beta_0 + \beta_1 \cdot \delta_x
$$

with (assuming that $\delta_{bgrd}$ is relatively close to 0): 

$$
\begin{aligned}
\beta_{1} &= \left( 1 - \frac{A_{bgrd}}{A_{peak}} \right) \cdot \frac{R_{VSMOW}}{R_{H2}} \\
\beta_{0} &= \frac{A_{bgrd}}{A_{peak}} \cdot \delta_{bgrd} \cdot \frac{R_{VSMOW}}{R_{H_2}} + \frac{R_{VSMOW}}{R_{H_2}} - 1 \\
\rightarrow \delta_{H_2} &= 
  - \frac{\beta_0 + \delta_{bgrd} \cdot (1 - \beta_1)
  }{\beta_0 + 1 + \delta_{bgrd} \cdot \beta_1} \approx - \frac{\beta_0}{\beta_0 + 1}\\
\rightarrow A_{bgrd} &= A_{peak} \cdot
  \frac{\beta_0 + 1 - \beta_1}
  {\beta_0 + 1 + \delta_{bgrd} \cdot \beta_1} \approx
  A_{peak} \cdot \left( 1 - 
  \frac{\beta_1}
  {\beta_0 + 1 } \right)
\end{aligned}
$$

First glance at the standards using this relationship and calculting $\delta_{H2}$ and $A_{bgrd}$ can be very useful to identify weird standards that deserve a second look.

## Step 1: $\delta_{peak/H_2} = \beta_0 + \beta_1 / A_{peak}$

Express the above mass balance as a univariate linear regression for each individual standard compound `x` (with known $\delta_x$):

$$
\delta_{peak/H_2} = \beta_0 + \beta_1 \cdot A_{peak}^{-1} 
$$

with: 

$$
\begin{aligned}
\beta_{0,x} &= ( \delta_{x} + 1)  \cdot \frac{R_{VSMOW}}{R_{H2}} - 1 \\
\beta_{1,x} &= A_{bgrd} \cdot \left( \delta_{bgrd} - \delta_{x} \right) \cdot \frac{R_{VSMOW}}{R_{H2}} \\
&= A_{bgrd} \cdot \left( \delta_{bgrd} - \delta_{x} \right) \cdot \frac{\beta_{0,x} + 1}{\delta_{x} + 1}
\end{aligned}
$$

## Step 2: $\delta_{H_2}$

Use the intercepts ($\beta_{0,x}$) to infer a $\delta_{H_2,x}$ for each standard compound `x` and incorporate the known error of $\delta_x$ and the measures error of intercept $\beta_{0,x}$. The resulting $\delta_{H_2,x}$ estimates are then used to calculate the $\delta_{H_2}$ for the entire analytical session as well as its error (either as standard deviation or standard error of the mean depending on the spread of $\beta_{0,x}$). This is similar to how this is approached in Polissar & d'Andrea except from a regression vs $A_{peak}$.

$$
\begin{aligned}
\delta_{H_2,x} &= \frac{\delta_{x} + 1}{\beta_{0,x} + 1} - 1 \\
\sigma_{\delta_{H_2,x}} &= \left(\delta_{H_2,x} + 1\right) \cdot \sqrt{
  \left(\frac{\sigma_{\beta_{0,x}}}{\beta_{0,x} + 1}\right)^2 +     
  \left(\frac{\sigma_{\delta_{x}}}{\delta_{x} + 1}\right)^2
}
\end{aligned}
$$

## Step 3: $y_x = \gamma_0 + \gamma_1 \cdot \delta_x$

Use all $\beta_{0,x}$ and $\beta_{1,x}$ values for a second linear regression derived from rearranging the equation for $\beta_{1,x}$ to yield:

$$
\begin{aligned}
\beta_{1,x} \cdot \frac{\delta^x + 1}{\beta_{0,x} + 1} &= A_{bgrd} \cdot \delta_{bgrd} - A_{bgrd} \cdot \delta_{x }\\
\rightarrow y_x &= \gamma_0 + \gamma_1 \cdot \delta_x
\end{aligned} 
$$

with

$$
\begin{aligned}
y_x &= \beta_{1,x} \cdot \frac{\delta^x + 1}{\beta_{0,x} + 1} \\
\gamma_0 &= A_{bgrd} \cdot \delta_{bgrd} \\
\gamma_1 &= -A_{bgrd} 
\end{aligned}
$$

## Step 4: $A_{bgrd}$, $\delta_{bgrd}$

Use the resulting regression parameters $\gamma_0$ (intercept) and $\gamma_1$ (slope) to calculate $A_{bgrd}$ and $\delta_{bgrd}$:

$$
\begin{aligned}
\rightarrow A_{bgrd} &= -\gamma_1 \\
\rightarrow \delta_{bgrd} &= -\frac{\gamma_0}{\gamma_1} \\
\sigma_{A_{bgrd}} &= \sigma_{\gamma_1} \\
\sigma_{\delta_{bgrd}} &= 
  |\delta_{bgrd}| \cdot \sqrt{
  \left(\frac{\sigma_{\gamma_0}}{\gamma_0}\right)^2 +     
  \left(\frac{\sigma_{\gamma_1}}{\gamma_1}\right)^2
}
\end{aligned}
$$

## Step 5: non-linear-least squares

Uses the parameters determined in steps 1-4 as starting values for a non-linear least squares fit of the mass balance equation with standards in the area range of interest (the smaller the range, the better the fit) to refine the parameter values for your calibration.

## Step 6: apply calibration

Solve the mass balance for $\delta_x$ ($\delta_{analyte/VSMOW}$) and calculate the calibration error $\sigma_{\delta_x}$ by standard error propagation:

$$
\begin{aligned}
\delta_x &= \frac{A_{peak}}{A_{peak} - A_{bgrd}} \cdot
  \left( \delta_{H_2} + \delta_{peak/H_2} + \delta_{H_2} \cdot \delta_{peak/H_2} \right) -
  \frac{A_{bgrd}}{A_{peak} - A_{bgrd}} \cdot \delta_{bgrd} \\
\frac{\partial \delta_x}{\partial \delta_{H_2}} &= 
  \frac{A_{peak}}{A_{peak} - A_{bgrd}} \cdot
  \left( 1 + \delta_{peak/H_2} \right) \\
\frac{\partial \delta_x}{\partial \delta_{bgrd}} &= 
  - \frac{A_{bgrd}}{A_{peak} - A_{bgrd}} \\
\frac{\partial \delta_x}{\partial A_{bgrd}} &= 
  \frac{A_{peak} \cdot 
    \left( \delta_{H_2} + \delta_{peak/H_2} + \delta_{H_2} \cdot \delta_{peak/H_2} - \delta_{bgrd} \right)}
    {(A_{peak} - A_{bgrd})^2} \\
\sigma_{\delta_x} &= \sqrt{
  \left(\frac{\partial \delta_x}{\partial \delta_{H_2}} \cdot \sigma_{\delta_{H_2}}\right)^2 +
  \left(\frac{\partial \delta_x}{\partial \delta_{bgrd}} \cdot \sigma_{\delta_{bgrd}}\right)^2 +
  \left(\frac{\partial \delta_x}{\partial A_{bgrd}} \cdot \sigma_{A_{bgrd}}\right)^2
}
\end{aligned}
$$

## Step 7: calculate averages and errors

This step entails calculating the analytical averages for sample compounds and their error estimates by evaluating propagated error from the calibration, pooled statistical error from the samples themselves, as well as unbiased standard deviation of the nC36 standard. Before this step it may be advisable to iterate back to Step 1 and re-run the calibration with an area limit if necessary as well as set the area range for where the calibration residuals are approximately zero.

## Step 8: correct for derivatization/hydrogenation

$$
\begin{aligned}
(nH_{original} + nH_{added}) \cdot \delta_x &= nH_{original} \cdot \delta_{corrected} + nH_{added} \cdot \delta_{added} \\
\rightarrow \delta_{corrected} &= \left(1 + \frac{nH_{added}}{nH_{original}} \right) \cdot \delta_x - \frac{nH_{added}}{nH_{original}} \cdot \delta_{added} \\
\rightarrow \sigma_{\delta_{corrected}} &= \sqrt{
  \left(1 + \frac{nH_{added}}{nH_{original}} \right)^2 \cdot \sigma_x^2 + 
   \left( \frac{nH_{added}}{nH_{original}} \right)^2 \cdot \sigma_{added}^2  
}
\end{aligned}
$$

# Load peak tables (*)

This table is generated from multiple analytical sessions in `gc_irms_data_peak_mapping.qmd`.

```{r}
# load parquet file
peak_table <- arrow::read_parquet("data/combined_peak_table.parquet")
```

# Prepare calibration (*)

```{r}
# set area limit for calibration parameter calculation (with sequential lms)
area_limits <- 
  tibble::tribble(
    ~session, ~area_limit.Vs,
    "2023-06", Inf,
    "2024-06", 40
  )
area_limits |> knitr::kable()

# use this area range for where to apply the calibration, will use
# the cloesest larger/smaller to these values from the calibration
# (used for nls calibration)
calib_area_range <- 
  tibble::tribble(
    ~session, ~area_calib_min.Vs, ~area_calib_max.Vs,
    "2023-06", 3,           40,
    "2024-06", 3,           40
  )
calib_area_range |> knitr::kable()

# define calibration standards
calculate_unbiased_sd <- function(sd, n) {
  cn <- suppressWarnings(sqrt(2/(n - 1)) * gamma(n/2) / gamma((n - 1) / 2))
  return(sd/cn)
}
standards <- 
  readxl::read_excel("data/standards.xlsx", "A7-2025") |>
  # nC16 is the first peak and usually systematically off
  filter(!compound %in% c("nC16")) |>
  mutate(
    known_d2H_sem.permil = 
      calculate_unbiased_sd(known_d2H_sd.permil, n_d2H)/sqrt(n_d2H),
    .after = "known_d2H_sd.permil"
  ) |> select("compound", "n_d2H", "known_d2H.permil", "known_d2H_sem.permil") 
standards |> knitr::kable(d = 3)

# merge info into peak table
peak_table_for_calib <- 
  peak_table |>
  filter(!is.na(d2H_vs_H2.permil)) |>
  left_join(area_limits, by = "session") |>
  relocate("area_limit.Vs", .after = "area.Vs") |>
  left_join(
    standards |> mutate(type = "standard"), 
    by = c("type", "compound")
  ) |>
  # flag standards for calibration
  # (include those within the area limit with known isotopic composition)
  mutate(
    is_standard = type == "standard" & !is.na(known_d2H.permil),
    use_to_calibrate = is_standard & area.Vs <= area_limit.Vs
  )
stopifnot("area limits have missing sessions" =
            all(!is.na(peak_table_for_calib$area_limit.Vs)))
```

# Calibrate

## Step 0: $\delta_{peak/H_2} = \beta_0 + \beta_1 \cdot \delta_x$

### Calculation

```{r}
#| label: calc-s0-reg-each-standard
# regression per standard
each_std_reg <- 
  peak_table_for_calib |> 
  filter(is_standard) |>
  nest(data = -c("session", "analysis")) |>
  mutate(
    n_peaks = map_int(data, nrow),
    A_mean.Vs = map_dbl(data, ~mean(.x$area.Vs, na.rm = TRUE)),
    A_sd.Vs = map_dbl(data, ~sd(.x$area.Vs, na.rm = TRUE)),
    fit = map(data, lm, formula = d2H_vs_H2.permil ~ known_d2H.permil),
    coefs = map(fit, broom::tidy),
    stats = map(fit, broom::glance)
  ) |>
  select(-"data", -"fit") |>
  unnest("coefs") |>
  select(-"statistic", -"p.value") |>
  unnest("stats") |>
  mutate(
    RMSE_v.permil = sqrt(deviance/nobs),
    term = case_when(term == "(Intercept)" ~ "intercept.permil", 
                     term == "known_d2H.permil" ~ "slope"),
    .after = "n_peaks"
  ) |>
  select(1:"std.error") |>
  rename(v = estimate, sem = std.error) |>
  # pivot wider for calculations
  pivot_wider(
    names_from = term, values_from = c(v, sem), 
    names_glue = "{gsub('\\\\..*', '', term)}_{.value}{gsub('^[^.]*', '', term)}"
  ) |>
  # calculate derived quantities
  mutate(
    d2H_H2_v.permil = -intercept_v.permil/(intercept_v.permil/1000 + 1),
    d2H_H2_sem.permil = abs(intercept_sem.permil * d2H_H2_v.permil/intercept_v.permil),
    A_bgrd_v.Vs = A_mean.Vs * (1 - slope_v / (intercept_v.permil/1000 + 1)),
    A_bgrd_sem.Vs = sqrt(
      (intercept_sem.permil/1000 * A_mean.Vs * slope_v / (intercept_v.permil/1000 + 1)^2)^2 +
        (slope_sem * A_mean.Vs / (intercept_v.permil/1000 + 1))^2 +
        (A_sd.Vs * (1 - slope_v / (intercept_v.permil/1000 + 1)))^2
    )
  )

each_std_reg |> head() |> knitr::kable(d = 2)
```

### Visualization

```{r}
#| label: fig-s0-reg-each-standard
#| fig-width: 16
#| fig-height: 11
each_std_reg |>
  select("session", "analysis", "n_peaks", matches("A_|RMSE|d2H_H2|A_bgrd")) |>
  pivot_longer(
    matches("RMSE|d2H_H2|A_bgrd"),
     names_to = c("parameter", ".value", "units"),
    names_pattern = "(.+)_(v|sem)\\.(.*)" 
  ) |>
  mutate(
    parameter = parameter |> as_factor() |>
      fct_recode(
        "phantom()%~~%A['bgrd']" = "A_bgrd",
        "phantom()%~~%delta^2*H['ref']" = "d2H_H2"
      ),
    panel = sprintf("%s~'[%s]'", parameter, str_replace(units, "permil", "\U2030")) |> 
      as_factor(),
    session_panel = sprintf("'%s'", session)
  ) |>
  left_join(calib_area_range, by = "session") |>
  filter(!is.na(v)) |>
  ggplot() +
  aes(A_mean.Vs, v, color = session, label = analysis) + 
  # background
  geom_rect(
    data = ~.x |> select("session", "area_calib_min.Vs", "area_calib_max.Vs") |>
      distinct(),
    map = aes(
      x = NULL, xmin = area_calib_min.Vs, xmax = area_calib_max.Vs, 
      y = NULL, ymin = -Inf, ymax = Inf, label = NULL), color = NA, fill = "gray90"
  ) +
  # averages (for those in area)
  geom_hline(
    data = ~.x |> filter(
      !str_detect(panel, "RMSE"), 
      A_mean.Vs >= area_calib_min.Vs, A_mean.Vs <= area_calib_max.Vs) |> 
      summarize(.by = c("session_panel", "session", "panel"), mv = mean(v)),
    map = aes(yintercept = mv, color = session)
  ) +
  geom_errorbarh(
    data = ~.x |> filter(A_mean.Vs >= area_calib_min.Vs, A_mean.Vs <= area_calib_max.Vs),
    map = aes(xmin = A_mean.Vs - A_sd.Vs, xmax = A_mean.Vs + A_sd.Vs), 
    height = 0, alpha = 0.4, show.legend = FALSE) +
  geom_errorbar(
    data = ~.x |> filter(A_mean.Vs >= area_calib_min.Vs, A_mean.Vs <= area_calib_max.Vs),
    map = aes(ymin = v - sem, ymax = v + sem), 
    width = 0, alpha = 0.4, show.legend = FALSE) +
  geom_point(
    data = ~.x |> filter(A_mean.Vs < area_calib_min.Vs | A_mean.Vs > area_calib_max.Vs),
    map = aes(color = NULL), color = "gray", size = 2
  ) +
  geom_point(
    data = ~.x |> filter(A_mean.Vs >= area_calib_min.Vs, A_mean.Vs <= area_calib_max.Vs),
    size = 2
  ) +
  scale_x_log10(breaks = scales::breaks_log(8)) +
  scale_y_continuous(breaks = scales::breaks_pretty(6)) +
  scale_color_brewer(palette = "Dark2") +
  facet_grid(panel ~ session_panel, labeller = ggplot2::label_parsed, 
             scales = "free", switch = "y") +
  theme_figure() +
  theme(strip.background.y = element_blank(), strip.placement = "outside") +
  labs(x = expression(bar(A)['peaks']~"[Vs]"), y = NULL)
```


### Visualization (is this useful?)

```{r}
# resulting bgrd and d_H2 estimates
each_std_params <- 
  each_std_reg |>
  filter(.by = "session", n_peaks == max(n_peaks)) |>
  summarize(
    .by = "session",
    n_stds = n(),
    n_peaks = max(n_peaks),
    coefs = 
      nls(b1_v ~ 1/(d2H_H2.permil/1000 + 1) 
          - 1/(d2H_H2.permil/1000 + 1) * A_bgrd.Vs/area_mean.Vs,
          start = list(d2H_H2.permil = 0, A_bgrd.Vs = 0)) |> 
      broom::tidy() |> list()
  ) |> unnest("coefs") |>
  select(-"statistic", -"p.value") |>
  rename(v = "estimate", sem = "std.error") |>
  pivot_wider(
    names_from = term, values_from = c(v, sem), 
    names_glue = "{gsub('\\\\..*', '', term)}_{.value}{gsub('^[^.]*', '', term)}"
  )

each_std_params |> knitr::kable(d = 3)
```



```{r}
breaks <- c(20, 10, 7, 5:1)

each_std_reg |> 
  filter(n_peaks == max(n_peaks)) |>
  left_join(each_std_params, by = "session") |>
  mutate(
    label = sprintf(
      "%s\nδH2: %s\U00B1%s\U2030\nbgrd: %s\U00B1%sVs",
      session, round(d2H_H2_v.permil, 1), round(d2H_H2_sem.permil, 1),
      round(A_bgrd_v.Vs, 3), round(A_bgrd_sem.Vs, 3))
  ) |>
  ggplot() +
  aes(x = 1/area_mean.Vs, y = b1_v,
      #y = residual, 
      color = label) +
  geom_abline(
    data = ~.x |>
      summarize(.by = "label", icept = 1/(d2H_H2_v.permil[1]/1000 + 1), 
                slope = - A_bgrd_v.Vs[1]/(d2H_H2_v.permil[1]/1000 + 1)) |>
      distinct(),
    map = aes(intercept = icept, slope = slope, color = label),
    linewidth = 1.5
  ) +
  geom_errorbar(
    map = aes(ymin = b1_v - b1_sem, ymax = b1_v + b1_sem), width = 0,
    alpha = 0.2, show.legend = FALSE
  ) +
  geom_errorbarh(
    map = aes(xmin = 1/(area_mean.Vs - area_sd.Vs),
              xmax = 1/(area_mean.Vs + area_sd.Vs)), 
    height = 0, alpha = 0.2, show.legend = FALSE
  ) +
  geom_point(size = 2, alpha = 1) +
  scale_x_log10() +
  scale_x_continuous(
    breaks = scales::breaks_pretty(8),
    sec.axis = sec_axis(
      transform = ~1/.x,
      breaks = breaks
    )
  ) +
  scale_color_brewer(palette = "Dark2") +
  theme_figure() +
  theme(legend.key.spacing.y = unit(0.2, "npc")) +
  labs(color = NULL, x = "area [Vs]", y = expression(beta[1]))
```


## Step 1: $\delta_{peak/H_2} = \beta_0 + \beta_1 / A_{peak}$

### Calculation (*)

```{r}
#| label: calc-s1-reg-each-compound
each_cmp_reg <- 
  peak_table_for_calib |> 
  filter(use_to_calibrate) |>
  nest(data = -c("session", "compound")) |>
  mutate(
    n_stds = map_int(data, nrow),
    fit = map(data, lm, formula = d2H_vs_H2.permil ~ I(1/area.Vs)),
    coefs = map(fit, broom::tidy)
  ) |>
  select(-"data", -"fit") |>
  unnest("coefs") |>
  select(-"statistic", -"p.value") |>
  mutate(term = case_when(term == "(Intercept)" ~ "b0", term == "I(1/area.Vs)" ~ "b1")) |>
  rename(v = estimate, sem = std.error) |>
  pivot_wider(names_from = term, values_from = c(v, sem), 
              names_glue = "{term}_{.value}")

each_cmp_reg |> knitr::kable(d = 2)
```

### Visualization

```{r}
#| label: fig-s1-reg-each-compound
#| fig-width: 13
#| fig-height: 11
peak_table_for_calib |> 
  filter(is_standard) |>
  mutate(
    panel = sprintf("%s (%s\U2030)", compound, known_d2H.permil)
  ) |>
  ggplot() +
  aes(x = area.Vs, y = d2H_vs_H2.permil, color = session, shape = session) +
  geom_point(
    data = ~filter(.x, use_to_calibrate), alpha = 1) +
  geom_point(
    data = ~filter(.x, !use_to_calibrate), alpha = 0.3) +
  geom_smooth(
    data = ~filter(.x, use_to_calibrate),
    method = "lm", formula = y ~ I(1/x), show.legend = FALSE
  ) +
  facet_wrap(~panel, scales = "free") +
  scale_color_brewer(palette = "Dark2") +
  theme_figure() +
  guides(color = guide_legend(override.aes = list(alpha = 1, size = 3))) +
  labs(x = "area [Vs]", y = expression("\U03B4"[peak/H[2]]~"[\U2030]")) 
```

## Step 2: $\delta_{H_2}$

### Calculation (*)

```{r}
#| label: calc-s2-ref-H2
# calculate d2H of H2 ref gas
each_cmp_params <- 
  each_cmp_reg |>
  left_join(standards |> select("compound", matches("d2H")), by = "compound") |>
  mutate(
    d2H_H2.permil = 1000 * ((known_d2H.permil/1000 + 1) / (b0_v/1000 + 1) - 1),
    d2H_H2_sem.permil = abs(d2H_H2.permil + 1000) * sqrt(
      (b0_sem/(b0_v + 1000))^2 + (known_d2H_sem.permil/(known_d2H.permil + 1000))^2
    ),
    y.Vs = b1_v * (known_d2H.permil + 1000)/(b0_v + 1000),
    y_sem.Vs = abs(y.Vs) * sqrt(
      (b1_sem/b1_v)^2 + (b0_sem/(b0_v + 1000))^2 + (known_d2H_sem.permil/(known_d2H.permil + 1000))^2
    )
  )

# what then is the ref gas uncertainty? (Polissar & d'Andrea)
refH2 <- 
  each_cmp_params |>
  bind_rows(each_cmp_params |> mutate(session = "all")) |>
  summarise(
    .by = "session",
    # sem based on the means of the sds
    d_H2_sem.permil = mean(d2H_H2_sem.permil)/sqrt(n()),
    # sd based on the measured means
    d_H2_sd.permil = sd(d2H_H2.permil),
    # which one to use? see discussion in Polissar & d'Andrea
    case = if_else(
      d_H2_sd.permil <= d_H2_sem.permil * sqrt(n()), "case 1 (sem)", "case 2 (sd)"
    ),
    # mean
    d_H2_v.permil = mean(d2H_H2.permil),
    # effective error to use
    d_H2_err.permil = case_when(
      case == "case 1 (sem)" ~ d_H2_sem.permil,
      case == "case 2 (sd)" ~ d_H2_sd.permil
    )
  )
refH2 |> knitr::kable(d = 1)
```

### Visualization

```{r}
#| label: fig-s2-ref-H2
#| fig-width: 12
#| fig-height: 5

# visualize
set.seed(123)
each_cmp_params |>
  filter(session != "all") |>
  ggplot() +
  aes(x = compound, y = d2H_H2.permil, 
      color = session, shape = session,
      ymin = d2H_H2.permil - d2H_H2_sem.permil, 
      ymax = d2H_H2.permil + d2H_H2_sem.permil) + 
  # mean
  geom_hline(
    data = refH2 |> filter(session != "all"), 
    map = aes(yintercept = d_H2_v.permil, color = session, linetype = "mean")
  ) +
  # error ranges
  geom_hline(
    data = refH2 |> 
      filter(session != "all") |>
      pivot_longer(matches("sem|sd")) |>
      crossing(pm = c(-1, 1)) |>
      mutate(
        name = str_extract(name, "sem|sd"),
        value = d_H2_v.permil + pm * value
      ),
    map = aes(yintercept = value, linetype = name, color = session)
  ) +
  # data
  geom_pointrange(size = 1, position = "jitter") +
  scale_color_brewer(palette = "Dark2") +
  theme_figure(grid = FALSE) +
  labs(x = NULL, y = expression("\U03B4 H"[2]~"[\U2030]"), 
       fill = "used", linetype = "option")
```

## Step 3: $y_x = \gamma_0 + \gamma_1 \cdot \delta_x$

### Calculation (*)

```{r}
#| label: calc-s3-reg-y-vs-delta
y_vs_delta_reg <- 
  each_cmp_params |>
  nest(data = -c("session")) |>
  mutate(
    n_compounds = map_int(data, nrow),
    # should this be a york regression instead?
    # seems unnecessary given the tiny errors in x
    fit = map(data, lm, formula = y.Vs ~ known_d2H.permil),
    coefs = map(fit, broom::tidy)
  ) |>
  select(-"data", -"fit") |> unnest("coefs") |>
  select(-"statistic", -"p.value") |>
  mutate(term = case_when(term == "(Intercept)" ~ "g0", term == "known_d2H.permil" ~ "g1")) |>
  rename(v = estimate, sem = std.error) |>
  pivot_wider(names_from = term, values_from = c(v, sem), 
              names_glue = "{term}_{.value}")

y_vs_delta_reg |> knitr::kable(d = 3)
```

### Visualization

```{r}
#| label: fig-s3-reg-y-vs-delta
#| fig-width: 8
#| fig-height: 5

# visualize
each_cmp_params |>
  ggplot() + 
  aes(x = known_d2H.permil, color = session, shape = session,
      xmin = known_d2H.permil - known_d2H_sem.permil,
      xmax = known_d2H.permil + known_d2H_sem.permil,
      y = y.Vs, ymin = y.Vs - y_sem.Vs, ymax = y.Vs + y_sem.Vs,
      label = compound
  ) +
  # intercept
  geom_hline(
    data = y_vs_delta_reg,
    map = aes(
      yintercept = g0_v, color = session, 
      linetype = "gamma[0] == A[bgrd] %.% delta[bgrd]")) +
  # slope
  geom_smooth(
    map = aes(xmin = NULL, xmax = NULL, label = NULL, 
              linetype = "gamma[1] == -A[bgrd]"), 
    method = "lm", formula = y ~ x, fullrange = TRUE
  ) +
  geom_errorbarh(height = 0, show.legend = FALSE) +
  geom_errorbar(width = 0, show.legend = FALSE) +
  geom_point(size = 4) +
  theme_figure(grid = FALSE) +
  scale_linetype_manual(values = c(2, 1), labels = scales::label_parse()) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  guides(
    color = guide_legend(override.aes = list(fill = NA)),
    linetype = guide_legend(override.aes = list(fill = NA, color = "black"))
  ) +
  expand_limits(
    x = extend_limits(c(standards$known_d2H.permil, y_vs_delta_reg$g0_v), mult = 0.05)) +
  coord_cartesian(expand = FALSE) +
  labs(
    x = expression("\U03B4"[known]~"[\U2030]"),
    y = expression(beta[1] %.% over("\U03B4"[known] + 1, beta[0] + 1)~"[Vs]"),
    linetype = "parameters"
  )
```

```{r}
#| label: fig-s3-reg-y-vs-delta-interactive
#| fig-width: 8
#| fig-height: 5
#| eval: false
#| echo: false

# visualize
(ggplot2::last_plot() + labs(x = "known d2H", y = "y")) |> 
  plotly::ggplotly(dynamicTicks = TRUE)
```

## Step 4: $A_{bgrd}$, $\delta_{bgrd}$

### Calculation (*)

```{r}
#| label: calc-s4-bgrd
# calculate the background parameters
bgrd <-
  y_vs_delta_reg |>
  transmute(
    session = session,
    A_bgrd_v.Vs = -g1_v, 
    A_bgrd_sem.Vs = g1_sem,
    A_bgrd_sd.Vs = g1_sem * sqrt(n()),
    d_bgrd_v.permil = -g0_v/g1_v,
    d_bgrd_sem.permil = abs(d_bgrd_v.permil) * sqrt( (g0_sem/g0_v)^2 + (g1_sem/g1_v)^2 ),
    d_bgrd_sd.permil = d_bgrd_sem.permil * sqrt(n())
  )

# actual parameters - which errors to use?
all_parameters <- 
  refH2 |> 
  select("session", "d_H2_v.permil", "d_H2_err.permil") |>
  left_join(bgrd, by = "session") |>
  # similar argument to with d_H2, we want to recognize there is probably
  # enough variability that the SEM may not be representative enough
  mutate(A_bgrd_err.Vs = A_bgrd_sd.Vs, .after = "A_bgrd_v.Vs") |>
  mutate(d_bgrd_err.permil = d_bgrd_sd.permil, .after = "d_bgrd_v.permil") |>
  select("session", matches("_(v|err)"))
all_parameters |> knitr::kable(d = 3)
```

### Visualization

```{r}
#| label: fig-s4-bgrd
#| fig-width: 12
#| fig-height: 3

# visualize all parameters
all_parameters |>
  filter(session != "all") |>
  pivot_longer(
    -"session", names_to = c("parameter", ".value", "units"),
    names_pattern = "([^_]+_[^_]+)_([^.]+)\\.(.*)") |>
  mutate(
    panel = sprintf("%s [%s]", parameter, units) |> as_factor(),
  ) |>
  ggplot() +
  aes(v, xmin = v-err, xmax = v+err,
      y = session, color = session) +
  geom_pointrange(size = 1) +
  scale_color_brewer(palette = "Dark2") +
  facet_grid(.~panel, scales = "free_x") +
  theme_figure(grid = FALSE) +
  theme(panel.spacing.x = unit(0.05, "npc")) +
  labs(x = NULL, y = NULL)
```


## Step 5: non-linear-least squares

### Calculation (*)

```{r}
#| label: calc-s5-nls-fit

# fit to model of the calibration
nls_parameters <-
  all_parameters |>
  filter(session != "all") |>
  left_join(
    peak_table_for_calib |> 
      # calibration peaks
      filter(use_to_calibrate) |>
      # only in relevant area range
      left_join(calib_area_range, by = "session") |>
      filter(area.Vs >= area_calib_min.Vs & area.Vs <= area_calib_max.Vs) |>
      nest(data = -c("session")),
    by = "session"
  ) |>
  mutate(
    .by = "session",
    coefs = 
      nls(
        known_d2H.permil ~
          area.Vs/(area.Vs - A_bgrd.Vs) *
          (d2H_vs_H2.permil + d_H2.permil + d2H_vs_H2.permil * d_H2.permil/1000) -
          A_bgrd.Vs/(area.Vs - A_bgrd.Vs) * d_bgrd.permil,
        # data set
        data = data[[1]],
        start =
          c(
            d_H2.permil = d_H2_v.permil,
            A_bgrd.Vs = A_bgrd_v.Vs,
            d_bgrd.permil = d_bgrd_v.permil
          )
        #lower = rep(0.1, nrow(norm_factors) - 1L) |> setNames(norm_factors$`Raw file`[-1]),
        #algorithm = "lmaccel" # this is a little faster than plain old "lm" but has the same result
        #algorithm = "plinear"
      ) |> summary() |> 
      coefficients() |> as.data.frame() |> 
      rownames_to_column() |> as_tibble() |> list()
  ) |>
  select("session", "coefs") |> unnest("coefs") |>
  select("session", "term" = "rowname", v = Estimate, err = "Std. Error") |> 
  pivot_wider(
    names_from = term, values_from = c(v, err), 
    names_glue = "{gsub('\\\\..*', '', term)}_{.value}{gsub('^[^.]*', '', term)}"
  )

# combined parameters
combined_parameters <- 
  all_parameters |> mutate(fit = "lms", .after = 1L) |>
  bind_rows(nls_parameters |> mutate(fit = "nls")) |>
  arrange(session, fit)

combined_parameters |> knitr::kable(d = 3)
```


### Visualization

```{r}
#| label: fig-s5-params-nls-vs-lm
#| fig-width: 12
#| fig-height: 4

# visualize all parameters
combined_parameters |>
  filter(session != "all") |>
  pivot_longer(
    c(-"session", -"fit"), names_to = c("parameter", ".value", "units"),
    names_pattern = "([^_]+_[^_]+)_([^.]+)\\.(.*)") |>
  pivot_wider(
    names_from = "fit",
    values_from = c("v", "err")
  ) |>
  mutate(
    panel = sprintf("%s [%s]", parameter, units) |> as_factor(),
  ) |>
  ggplot() +
  aes(v_lms,  y = v_nls, 
      xmin = v_lms - err_lms, xmax = v_lms + err_lms,
      ymin = v_nls - err_nls, ymax = v_nls + err_nls,
     color = session) +
  geom_abline(color = "black", linetype = 2) +
  geom_errorbarh(height = 0) + geom_errorbar(width = 0) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Dark2") +
  facet_wrap(.~panel, scales = "free") +
  theme_figure(grid = FALSE) +
  theme(panel.spacing.x = unit(0.05, "npc")) +
  labs(x = "sequential linear least squares", y = "non-linear least square")
```


## Step 6: apply calibration

### Calculation (*)

```{r}
#| label: calc-s6-calibrated-data
# run the processing pipeline
peak_table_calibrated <- 
  peak_table_for_calib |>
  filter(!is.na(compound), !is.na(d2H_vs_H2.permil)) |> 
  left_join(calib_area_range, by = "session") |>
  left_join(
    bind_rows(
      combined_parameters,
      combined_parameters |> select("session") |> distinct() |>
        mutate(fit = "direct")
    ), 
    by = "session", relationship = "many-to-many") |>
  # this step is ONLY needed for correction without area consideration!
  mutate(
    .by = c("session", "fit", "compound"),
    d_H2_direct.permil = 
      mean(
      (known_d2H.permil[use_to_calibrate] + 1000) / 
      (d2H_vs_H2.permil[use_to_calibrate]/1000 + 1) - 1000
      )
  ) |>
  # regular calculations
  mutate(
    .by = c("session", "fit"),
    area_calib_min.Vs = min(area.Vs[use_to_calibrate & area.Vs >= area_calib_min.Vs]),
    area_calib_max.Vs = max(area.Vs[use_to_calibrate & area.Vs <= area_calib_max.Vs]),
    in_area_range = area.Vs >= area_calib_min.Vs & area.Vs <= area_calib_max.Vs,
    calibrated_d2H.permil = 
      case_when(
        fit == "direct" ~ 
          # without area consideration (i.e. standard correction) - just for comparison
          d2H_vs_H2.permil +
          mean(unique(d_H2_direct.permil[use_to_calibrate])) + 
          d2H_vs_H2.permil * mean(unique(d_H2_direct.permil[use_to_calibrate]))/1000,
        TRUE ~ 
          area.Vs/(area.Vs - A_bgrd_v.Vs) * 
          (d2H_vs_H2.permil + d_H2_v.permil + d2H_vs_H2.permil * d_H2_v.permil/1000) -
          A_bgrd_v.Vs/(area.Vs - A_bgrd_v.Vs) * d_bgrd_v.permil
      ),
    calibrated_d2H_err.permil =
      sqrt(
        (area.Vs/(area.Vs - A_bgrd_v.Vs) * (1 + d2H_vs_H2.permil/1000) * d_H2_err.permil)^2 +
          (A_bgrd_v.Vs/(area.Vs - A_bgrd_v.Vs) * d_bgrd_err.permil)^2 +
          (area.Vs/(area.Vs - A_bgrd_v.Vs)^2 * area.Vs *
             (d2H_vs_H2.permil + d_H2_v.permil + d2H_vs_H2.permil * d_H2_v.permil/1000 + d_bgrd_v.permil) *
             A_bgrd_err.Vs)^2
      ),
    residual.permil = if_else(
      !is.na(known_d2H.permil),
      calibrated_d2H.permil - known_d2H.permil,
      NA_real_
    )
  )

stopifnot("area range has missing sessions" =
            all(!is.na(peak_table_calibrated$area_calib_min.Vs)))

# whole calibration RMSE (within the calib area) 
# root mean square error / root mean square deviation
calib_stats <- 
  peak_table_calibrated |>
  bind_rows(peak_table_calibrated |> mutate(session = "all")) |>
  filter(use_to_calibrate, in_area_range) |>
  summarize(
    .by = c("session", "fit"),
    n_analyses = unique(file_id) |> length(),
    n_std_peaks = n(), 
    area_actual_min.Vs = min(area.Vs),
    area_actual_max.Vs = max(area.Vs),
    RMSE.permil = sqrt(mean(residual.permil^2)),
    resid_mean.permil = mean(residual.permil),
    resid_min.permil = min(residual.permil),
    resid_max.permil = max(residual.permil)
  )
calib_stats |> knitr::kable(d = 1)
```


### Visualization

#### Right: residual distribution

```{r}
# plot ranges
resid_plot_range.permil <- c(-20, 20)
area_plot_range.permil <- c(2, 100)
compounds <- "BP"
binwidth <- 2

# residual distribution plots
p_residual_distribution <-
  peak_table_calibrated |>
  filter(is_standard, in_area_range) |>
  summarize(
    .by = c("session", "fit"),
    right = list(
      ggplot(pick(everything())) +
        aes(y = residual.permil, x = after_stat(density)) +
        # low/high d2H standards
        geom_histogram(
          map = aes(
            fill = known_d2H.permil > median(known_d2H.permil),
            color = known_d2H.permil > median(known_d2H.permil)),
          alpha = 0.5, binwidth = binwidth, #bins = 30, 
          position = "identity", color = NA,
          show.legend = FALSE
        ) +
        # residual distribution
        geom_histogram(
          binwidth = binwidth, #bins = 30, 
          fill = NA_character_, color = "black",
          linewidth = 0.5
        )  +
        # normal distribution on top
        geom_path(
          data = ~.x |>
            reframe(
              y = seq(min(residual.permil), max(residual.permil), length.out = 1000),
              x = dnorm(y, mean = 0, 
                        sd = sd(residual.permil)) #* n() * diff(range(residual.permil))/bins
            ),
          map = aes(x = x, y = y), linewidth = 1
        ) +
        # rmse
        geom_text(
          data = ~.x |>
            summarize(
               RMSE.permil = sqrt(mean(residual.permil^2)),
               resid_mean.permil = mean(residual.permil)
            ),
          map = aes(x = Inf, y = Inf, label = 
            sprintf("RMSE: %.1f\U2030\nMean: %.1f\U2030", 
                    RMSE.permil, resid_mean.permil)),
          vjust = 1.5, hjust = 1.1, size = 4
        ) +
        # hline
        geom_hline(yintercept = 0, linetype = 2) +
        # styling
        scale_x_continuous(expand = expansion(mult = c(0, .05))) +
        scale_y_continuous(
          breaks = scales::breaks_pretty(8),
          sec.axis = dup_axis()) +
        scale_fill_manual(values = pals::warmcool(2)) +
        theme_figure(grid = FALSE) +
        theme(
          axis.ticks.x = element_blank(),
          axis.text.x = element_blank(),
          axis.ticks.y.left = element_blank(),
          axis.text.y.left = element_blank()
        ) +
        labs(x = NULL, y = NULL) +
        coord_cartesian(ylim = resid_plot_range.permil)
    )
  )
```

#### Center: residuals

```{r}
# create residuals plot
p_residuals <- 
  peak_table_calibrated |>
  filter(is_standard) |>
  mutate(panel = sprintf("%s (%s)", session, fit)) |>
  summarize(
    .by = c("session", "fit"),
    center = list(
      ggplot(pick(everything())) +
        aes(x = area.Vs, y = residual.permil) +
        # area calibration range
        geom_rect(
          data = ~.x |> select("area_calib_min.Vs", "area_calib_max.Vs") |>
            distinct(),
          map = aes(
            x = NULL, xmin = area_calib_min.Vs, xmax = area_calib_max.Vs, 
            y = NULL, ymin = -Inf, ymax = Inf), color = NA, fill = "gray90"
        ) +
        # standards' residuals
        geom_hline(yintercept = 0, linetype = 2) +
        geom_point(
          # FIXME: should the residual permil filter be here?
          data = ~.x |> filter(in_area_range), 
          map = aes(color = known_d2H.permil),
          alpha = 1
        ) + 
        geom_point(
          data = ~.x |> filter(!in_area_range), 
          alpha = 0.3
        ) + 
        # spread of residuals
        geom_smooth(
          data = ~.x |> filter(is_standard, in_area_range),
          mapping = aes(color = NULL), 
          method = "loess", formula = y ~ x,
          color = "black", linetype = 1, linewidth = 2
        ) +
        # scales
        scale_x_log10(breaks = scales::breaks_log(8)) +
        scale_y_continuous(breaks = scales::breaks_pretty(8)) +
        scale_color_gradientn(
          colours = pals::warmcool(100), guide = "colourbar",
          labels = function(x) paste0(x, "\U2030")
        ) +
        theme_figure(grid = FALSE) +
        theme(legend.position = "left") +
        facet_wrap(~panel) +
        labs(
          x = "area [Vs]", y = "standard residuals [\U2030]", 
          color = expression(delta^2*"H"["known"])) +
        coord_cartesian(ylim = resid_plot_range.permil, 
                        xlim = area_plot_range.permil)
    )
  )
```

#### Top: sample distribution

```{r}
# create sample distribution plot
p_samples <- 
  peak_table_calibrated |>
  filter(type != "standard", str_detect(compound, !!compounds)) |>
  summarize(
    .by = c("session", "fit"),
    top = list(
      ggplot(pick(everything())) +
        aes(x = area.Vs) +
        # area calibration range
        geom_rect(
          data = ~.x |> select("area_calib_min.Vs", "area_calib_max.Vs") |>
            distinct(),
          map = aes(
            x = NULL, xmin = area_calib_min.Vs, xmax = area_calib_max.Vs, 
            y = NULL, ymin = -Inf, ymax = Inf), color = NA, fill = "gray90"
        ) +
        # sample distribution with area
        geom_density(
          map = aes(y = after_stat(count), color = compound), alpha = 0.1
        ) +
        # scales
        scale_x_log10(breaks = scales::breaks_log(8), sec.axis = dup_axis()) +
        scale_color_brewer(palette = "Dark2") +
        theme_figure(grid = FALSE) +
        theme(
          axis.text.x.bottom = element_blank(),
          axis.ticks.x.bottom = element_blank(),
          axis.title.x.bottom = element_blank(),
        ) +
        labs(x = "area [Vs]", y = "distribution [#]", color = NULL) +
        coord_cartesian(xlim = area_plot_range.permil)
    )
  )
```

#### Complete plot

```{r}
#| label: fig-s6-calibration-residuals-and-range
#| fig-width: 30
#| fig-height: 16

# assemble full plot
p_full <- p_residual_distribution |>
  left_join(p_residuals, by = c("session", "fit")) |>
  left_join(p_samples, by = c("session", "fit")) |>
  mutate(
    .by = "session",
    empty = list(ggplot() + theme(panel.background = element_blank()) +
                   theme(plot.margin = margin(l = 0, b = 0))),
    n = row_number()
  ) |>
  mutate(
    .by = c("session", "fit"),
    ptop = list(list(
      top[[1]] + 
        theme(plot.margin = margin(r = 0, b = 0), legend.position = 
                if(n[1] == 1L) "left" else "none") +
        labs(x = NULL), 
      empty[[1]])),
    pbot = list(list(
      center[[1]] +
        theme(plot.margin = margin(r = 0, t = 0), legend.position = 
                if(n[1] == 1L) "left" else "none"), 
      right[[1]] + 
        theme(plot.margin = margin(l = 0, t = 0))))
  ) |>
  summarize(
    .by = "session", n = n(),
    plots = list(c(ptop, pbot))
  )

egg::ggarrange(
  plots = p_full$plots |> unlist(recursive = FALSE) |> unlist(recursive = FALSE),
  nrow = 2 * length(unique(p_full$session)), 
  widths = c(c(3.5, 1), rep(c(3, 1), p_full$n[1] - 1L)), 
  heights = rep(c(1, 3), nrow(p_full))
)
```

## Step 7: calculate averages and errors

### Calculation (*)

```{r}
#| label: calc-s7-stats
# unbiased sdevs for methodological standards (within the calib area)
calib_compounds_stats <- 
  peak_table_calibrated |>
  bind_rows(peak_table_calibrated |> mutate(session = "all")) |>
  filter(in_area_range, str_detect(peak_type, "(?<!iso-)std|standard")) |>
  summarize(
    .by = c("session", "fit", "compound", "peak_type", "known_d2H.permil"),
    n_analyses = n(),
    calib_mean.permil = mean(calibrated_d2H.permil),
    sdev_unbiased.permil = calculate_unbiased_sd(sd(calibrated_d2H.permil), n()),
    RMSE = sqrt(mean(residual.permil^2))
  )
calib_compounds_stats |> knitr::kable(d = 1)

# which sdevs to use? compare propagated error, pooled sample error, and nC36 (standard) error
sample_stats <- 
  peak_table_calibrated |>
  bind_rows(peak_table_calibrated |> mutate(session = "all")) |>
  filter(type != "standard", str_detect(compound, !!compounds), in_area_range) |>
  summarize(
    .by = c("session", "fit", "id1", "compound"),
    n_rep_analyses = n(),
    sdev = sd(calibrated_d2H.permil), # statistical sdev
    sem = sqrt(sum( (calibrated_d2H_err.permil/n_rep_analyses)^2)) # propagated error
  ) |>
  # calculate unbiased sdev
  mutate(
    .by = "fit",
    sdev_unbiased = 
      case_when(
        n_rep_analyses > 1 ~ calculate_unbiased_sd(sdev, n_rep_analyses),
        TRUE ~ NA_real_
      )
  ) |>
  # summarize across samples
  summarize(
    .by = c("session", "fit", "compound"),
    # all that have more than 1 analysis contribute to the sdev_pooled estimate
    n_samples = sum(n_rep_analyses > 1),
    sdev_propagated = mean(sem),
    sdev_pooled = sqrt( sum((n_rep_analyses - 1) * sdev_unbiased^2, na.rm = TRUE) / 
                          sum(n_rep_analyses - 1))
  ) |>
  left_join(
    calib_compounds_stats |> filter(compound == "nC36") |> 
      select("session", "fit", "n_nC36" = "n_analyses", 
             "sdev_nC36" = "sdev_unbiased.permil"),
    by = c("session", "fit")
  ) |>
  # figure out which sdev to use for error calculation (largest)
  mutate(
    .by = c("session", "fit", "compound"),
    use_sdev = max(c(sdev_propagated, sdev_pooled, sdev_nC36))
  ) |>
  arrange(desc(session), compound)
sample_stats |> knitr::kable(d = 1)

# calculate averages and error ranges for the samples
stdev_warning <- 3
add_warning <- function(warnings, condition, msg) {
  case_when(
    !is.na(warnings) & !is.na(condition) & condition ~ sprintf("%s + %s", warnings, msg),
    !is.na(warnings) & (is.na(condition) | !condition) ~ warnings,
    is.na(warnings) & !is.na(condition) & condition ~ msg,
    TRUE ~ NA_character_,
    .size = length(condition)
  )
}

# summary
peak_table_summary <- 
  peak_table_calibrated |>
  filter(type != "standard", str_detect(compound, !!compounds)) |>
  filter(fit == "lms") |>
  summarize(
    .by = c("id1", "fit", "compound"),
    n_analyses_in_area_range = sum(in_area_range) |> as.integer(),
    n_analyses_below_area_range = sum(area.Vs < area_calib_min.Vs) |> as.integer(),
    n_analyses_above_area_range = sum(area.Vs > area_calib_max.Vs) |> as.integer(),
    # calculate sdev
    calibrated_d2H_sdev.permil = 
      case_when(
        # more than 1 analysis in area range
        n_analyses_in_area_range > 1 ~ 
          sd(calibrated_d2H.permil[in_area_range]) |>
          calculate_unbiased_sd(n_analyses_in_area_range),
        # more than 1 analysis above area range
        n_analyses_above_area_range > 1 ~
          sd(calibrated_d2H.permil[area.Vs > area_calib_max.Vs]) |>
          calculate_unbiased_sd(n_analyses_above_area_range),
        # not enough to calculate sdev
        TRUE ~ NA_real_
      ),
    calibrated_d2H.permil = 
      case_when(
        # at least 1 analysis in area range
        n_analyses_in_area_range > 0 ~ 
          mean(calibrated_d2H.permil[in_area_range]),
        # at least 1 analysis above area range
        n_analyses_above_area_range > 0 ~
          mean(calibrated_d2H.permil[area.Vs > area_calib_max.Vs]),
        # all below area range (can't calculate safely)
        TRUE ~ NA_real_
      )
  ) |> 
  relocate("calibrated_d2H_sdev.permil", .after = "calibrated_d2H.permil") |>
  left_join(sample_stats |> filter(session == "all") |>
              select("fit", "compound", matches("sdev")), by = c("fit", "compound")) |>
  # calculate standard error of the mean based on the sdev to use
  mutate(
    calibrated_d2H_sem.permil = 
      case_when(
        n_analyses_in_area_range > 0 ~ 
          use_sdev/sqrt(n_analyses_in_area_range),
        n_analyses_above_area_range > 0 ~
          use_sdev/sqrt(n_analyses_above_area_range),
        TRUE ~ NA_real_
      ),
    .after = "calibrated_d2H_sdev.permil"
  ) |> 
  # raise flags if there are unexpected oddities
  mutate(
    warnings = NA_character_ |>
      add_warning(
        n_analyses_in_area_range == 1 | 
          (n_analyses_in_area_range == 0 & n_analyses_above_area_range == 1),
        "single measurement"
      ) |>
      add_warning(
        n_analyses_in_area_range == 0 & n_analyses_above_area_range > 0,
        "above area range"
      ) |>
      add_warning(
        calibrated_d2H_sdev.permil > stdev_warning * use_sdev,
        sprintf("sample sd > %s used sd", stdev_warning)
      )
  ) |>
  arrange(id1, compound, fit)

# export
export_to_excel(
  `analyses` = peak_table_calibrated,
  `summary` = peak_table_summary |> filter(fit == "nls"),
  file = "output/gc_irms_data_calibrated.xlsx"
)

# show a bit
peak_table_summary |> head()
```

### Visualization

```{r}
#| label: fig-s6-data-overview
#| fig-width: 12
#| fig-height: 6
#| warning: false
set.seed(123)
plot_df <- 
  peak_table_calibrated |>
  semi_join(peak_table_summary, by = c("fit", "compound", "id1")) |>
  filter(in_area_range) |>
  mutate(id1 = factor(id1))

plot_df |>
  ggplot() +
  aes(id1 |> as.integer(), calibrated_d2H.permil, color = compound,
      ymin = calibrated_d2H.permil - calibrated_d2H_err.permil,
      ymax = calibrated_d2H.permil + calibrated_d2H_err.permil) +
  # individual measurements
  geom_pointrange(position = "jitter", alpha = 0.3, map = aes(size = area.Vs)) +
  # averages
  geom_pointrange(
    data = peak_table_summary |> 
      filter(!is.na(calibrated_d2H.permil)) |>
      mutate(id1 = factor(id1), warnings = factor(warnings) |> 
               fct_na_value_to_level() |> 
               fct_recode("none" = NA_character_) |>
               fct_relevel("none")),
    map = aes(y = calibrated_d2H.permil, shape = warnings,
              ymin = calibrated_d2H.permil - calibrated_d2H_sem.permil,
              ymax = calibrated_d2H.permil + calibrated_d2H_sem.permil),
    position = "jitter", size = 1
  ) +
  scale_x_continuous(
    breaks = plot_df$id1 |> levels() |> seq_along(), expand = c(0, 0.1),
    labels = levels(plot_df$id1)
  ) +
  scale_color_brewer(palette = "Dark2") +
  scale_size_continuous(range = c(0.1, 1)) +
  scale_shape_manual(values = c(16:18, 15)) +
  theme_figure(axis_x_rotate = 90, grid = FALSE) +
  theme(panel.grid.minor.x = element_line(linetype = 2)) +
  labs(x = NULL, y = expression("calibrated "*delta^2*H~"[\U2030]"))
```

### Visualization: area correction check

```{r}
#| label: fig-s6-data-area-corrections
#| fig-width: 12
#| fig-height: 6
#| warning: false
peak_table_calibrated |>
  semi_join(
    peak_table_summary |> filter(n_analyses_in_area_range > 3),
    by = c("id1", "compound")
  ) |>
  filter(in_area_range, fit %in% c("lms", "direct")) |>
  filter(.by = "id1", length(unique(compound)) == 4) |>
  ggplot() +
  aes(x = area.Vs, y = calibrated_d2H.permil, color = compound, shape = fit) +
  geom_smooth(
    data = ~.x |> filter(fit == "direct"), fullrange = TRUE,
    method = "lm", formula = y ~ I(1/x), se = FALSE, show.legend = FALSE
  ) +
  # geom_errorbar(
  #   map = aes(ymin = calibrated_d2H.permil - calibrated_d2H_err.permil,
  #             ymax = calibrated_d2H.permil + calibrated_d2H_err.permil),
  #   width = 0, alpha = 0.5
  # ) +
  geom_hline(
    data = ~.x |> select("id1", "compound") |> distinct() |>
      left_join(peak_table_summary, by = c("id1", "compound")),
    map = aes(yintercept = calibrated_d2H.permil, color = compound)
  ) +
  geom_rect(
    data = ~.x |> select("id1", "compound") |> distinct() |>
      left_join(peak_table_summary, by = c("id1", "compound")),
    map = aes(
      ymin = calibrated_d2H.permil - calibrated_d2H_sem.permil,
      ymax = calibrated_d2H.permil + calibrated_d2H_sem.permil,
      x = NULL, xmin = -Inf, xmax = Inf,
      fill = compound, color = NULL
    ), alpha = 0.2
  ) +
  geom_line(map = aes(group = paste(id1, compound, area.Vs)), linetype = 2) +
  geom_point(size = 3) +
  geom_pointrange(
    data = ~.x |> filter(fit == "direct") |>
      mutate(max_area = max(area.Vs)) |>
      summarize(
        .by = c("id1", "compound", "max_area"), 
        x = lm(calibrated_d2H.permil ~ I(1/area.Vs)) |> 
          broom::tidy() |> filter(term == "(Intercept)") |> list()
      ) |> unnest(x) |> mutate(fit = "yintercept"),
    map = aes(x = max_area * 1.1, y = estimate,
              ymin = estimate - std.error, ymax = estimate + std.error),
    size = 1
  ) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_continuous(expand = expansion(mult = c(0, 0.02))) +
  facet_wrap(~id1, scales = "free") +
  theme_figure(grid = FALSE) +
  expand_limits(x = c(2.5))
```


## Step 8: correct for derivatization/hydrogenation

### Calculation(*)

```{r}
# from 18-O-18 tests with Pt+H2
added_d2H.permil <- -676.32	
added_d2H_sd.permil <- 134.39

peak_table_summary_final <-
  peak_table_summary |>
  mutate(
    nH_added = 2L,
    nH_original = 80L - 2L * abs(as.integer(parse_number(compound))),
    corrected_d2H.permil = (1 + nH_added/nH_original) * calibrated_d2H.permil - 
      nH_added/nH_original * added_d2H.permil,
    corrected_d2H_sem.permil = sqrt(
      ((1 + nH_added/nH_original) * calibrated_d2H_sem.permil)^2 +
      (nH_added/nH_original * added_d2H_sd.permil)^2
    )
  )

# added uncertainty:
peak_table_summary_final |> 
  filter(!is.na(calibrated_d2H.permil))  |>
  summarize(
    added_d2H.permil = added_d2H.permil,
    added_d2H_sd.permil = added_d2H_sd.permil,
    d2H_change_min.permil = min(corrected_d2H.permil - calibrated_d2H.permil),
    d2H_change_max.permil = max(corrected_d2H.permil - calibrated_d2H.permil),
    d2H_sem_change_min.permil = min(corrected_d2H_sem.permil - calibrated_d2H_sem.permil),
    d2H_sem_change_max.permil = max(corrected_d2H_sem.permil - calibrated_d2H_sem.permil)
  ) |> 
  pivot_longer(everything()) |>
  knitr::kable(d=1)

# export
export_to_excel(
  `analyses` = peak_table_calibrated,
  `summary` = peak_table_summary_final,
  file = "output/gc_irms_data_corrected.xlsx"
)
```

